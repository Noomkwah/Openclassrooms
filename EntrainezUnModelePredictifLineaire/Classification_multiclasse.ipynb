{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84c8dd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05d90892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acer_Opalus</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.035156</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pterocarya_Stenoptera</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.022461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quercus_Hartwissiana</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.068359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.154300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.002930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Tilia_Tomentosa</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Quercus_Variabilis</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.048828</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  species   margin1   margin2   margin3   margin4   margin5  \\\n",
       "id                                                                            \n",
       "1             Acer_Opalus  0.007812  0.023438  0.023438  0.003906  0.011719   \n",
       "2   Pterocarya_Stenoptera  0.005859  0.000000  0.031250  0.015625  0.025391   \n",
       "3    Quercus_Hartwissiana  0.005859  0.009766  0.019531  0.007812  0.003906   \n",
       "5         Tilia_Tomentosa  0.000000  0.003906  0.023438  0.005859  0.021484   \n",
       "6      Quercus_Variabilis  0.005859  0.003906  0.048828  0.009766  0.013672   \n",
       "\n",
       "     margin6   margin7  margin8   margin9  ...  texture55  texture56  \\\n",
       "id                                         ...                         \n",
       "1   0.009766  0.027344      0.0  0.001953  ...   0.007812   0.000000   \n",
       "2   0.001953  0.019531      0.0  0.000000  ...   0.000977   0.000000   \n",
       "3   0.005859  0.068359      0.0  0.000000  ...   0.154300   0.000000   \n",
       "5   0.019531  0.023438      0.0  0.013672  ...   0.000000   0.000977   \n",
       "6   0.015625  0.005859      0.0  0.000000  ...   0.096680   0.000000   \n",
       "\n",
       "    texture57  texture58  texture59  texture60  texture61  texture62  \\\n",
       "id                                                                     \n",
       "1    0.002930   0.002930   0.035156        0.0        0.0   0.004883   \n",
       "2    0.000000   0.000977   0.023438        0.0        0.0   0.000977   \n",
       "3    0.005859   0.000977   0.007812        0.0        0.0   0.000000   \n",
       "5    0.000000   0.000000   0.020508        0.0        0.0   0.017578   \n",
       "6    0.021484   0.000000   0.000000        0.0        0.0   0.000000   \n",
       "\n",
       "    texture63  texture64  \n",
       "id                        \n",
       "1    0.000000   0.025391  \n",
       "2    0.039062   0.022461  \n",
       "3    0.020508   0.002930  \n",
       "5    0.000000   0.047852  \n",
       "6    0.000000   0.031250  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On import les données d'entraînement.\n",
    "data = pd.read_csv(\"train.csv\", index_col=(0))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54c2cbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990, 193)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d3aae7e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>margin10</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019531</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.053711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.064453</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.033203</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006836</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.037109</td>\n",
       "      <td>0.044922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.036133</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042969</td>\n",
       "      <td>0.016602</td>\n",
       "      <td>0.010742</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     margin1   margin2   margin3   margin4   margin5   margin6   margin7  \\\n",
       "id                                                                         \n",
       "4   0.019531  0.009766  0.078125  0.011719  0.003906  0.015625  0.005859   \n",
       "7   0.007812  0.005859  0.064453  0.009766  0.003906  0.013672  0.007812   \n",
       "9   0.000000  0.000000  0.001953  0.021484  0.041016  0.000000  0.023438   \n",
       "12  0.000000  0.000000  0.009766  0.011719  0.017578  0.000000  0.003906   \n",
       "13  0.001953  0.000000  0.015625  0.009766  0.039062  0.000000  0.009766   \n",
       "\n",
       "    margin8   margin9  margin10  ...  texture55  texture56  texture57  \\\n",
       "id                               ...                                    \n",
       "4       0.0  0.005859  0.023438  ...   0.006836   0.000000   0.015625   \n",
       "7       0.0  0.033203  0.023438  ...   0.000000   0.000000   0.006836   \n",
       "9       0.0  0.011719  0.005859  ...   0.128910   0.000000   0.000977   \n",
       "12      0.0  0.003906  0.001953  ...   0.012695   0.015625   0.002930   \n",
       "13      0.0  0.005859  0.000000  ...   0.000000   0.042969   0.016602   \n",
       "\n",
       "    texture58  texture59  texture60  texture61  texture62  texture63  \\\n",
       "id                                                                     \n",
       "4    0.000977   0.015625        0.0        0.0   0.000000   0.003906   \n",
       "7    0.001953   0.013672        0.0        0.0   0.000977   0.037109   \n",
       "9    0.000000   0.000000        0.0        0.0   0.015625   0.000000   \n",
       "12   0.036133   0.013672        0.0        0.0   0.089844   0.000000   \n",
       "13   0.010742   0.041016        0.0        0.0   0.007812   0.009766   \n",
       "\n",
       "    texture64  \n",
       "id             \n",
       "4    0.053711  \n",
       "7    0.044922  \n",
       "9    0.000000  \n",
       "12   0.008789  \n",
       "13   0.007812  \n",
       "\n",
       "[5 rows x 192 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On importe les données de test.\n",
    "data_validation = pd.read_csv(\"test.csv\", index_col=(0))\n",
    "data_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5a76d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 192)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e62882",
   "metadata": {},
   "source": [
    "On voit qu'il manque une colonne dans le jeu de validation. Trouvons de quelle colonne il s'agit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0e4507f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'species'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data.columns).symmetric_difference(set(data_validation.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d43f6f",
   "metadata": {},
   "source": [
    "## Tour d'horizon des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96814899",
   "metadata": {},
   "source": [
    "On va s'intéresser exclusivement au jeu data dans ce TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1093031c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"species\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cddc35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>margin10</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>species</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Acer_Capillipes</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acer_Circinatum</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acer_Mono</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acer_Opalus</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Acer_Palmatum</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tilia_Tomentosa</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ulmus_Bergmanniana</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Viburnum_Tinus</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Viburnum_x_Rhytidophylloides</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zelkova_Serrata</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              margin1  margin2  margin3  margin4  margin5  \\\n",
       "species                                                                     \n",
       "Acer_Capillipes                    10       10       10       10       10   \n",
       "Acer_Circinatum                    10       10       10       10       10   \n",
       "Acer_Mono                          10       10       10       10       10   \n",
       "Acer_Opalus                        10       10       10       10       10   \n",
       "Acer_Palmatum                      10       10       10       10       10   \n",
       "...                               ...      ...      ...      ...      ...   \n",
       "Tilia_Tomentosa                    10       10       10       10       10   \n",
       "Ulmus_Bergmanniana                 10       10       10       10       10   \n",
       "Viburnum_Tinus                     10       10       10       10       10   \n",
       "Viburnum_x_Rhytidophylloides       10       10       10       10       10   \n",
       "Zelkova_Serrata                    10       10       10       10       10   \n",
       "\n",
       "                              margin6  margin7  margin8  margin9  margin10  \\\n",
       "species                                                                      \n",
       "Acer_Capillipes                    10       10       10       10        10   \n",
       "Acer_Circinatum                    10       10       10       10        10   \n",
       "Acer_Mono                          10       10       10       10        10   \n",
       "Acer_Opalus                        10       10       10       10        10   \n",
       "Acer_Palmatum                      10       10       10       10        10   \n",
       "...                               ...      ...      ...      ...       ...   \n",
       "Tilia_Tomentosa                    10       10       10       10        10   \n",
       "Ulmus_Bergmanniana                 10       10       10       10        10   \n",
       "Viburnum_Tinus                     10       10       10       10        10   \n",
       "Viburnum_x_Rhytidophylloides       10       10       10       10        10   \n",
       "Zelkova_Serrata                    10       10       10       10        10   \n",
       "\n",
       "                              ...  texture55  texture56  texture57  texture58  \\\n",
       "species                       ...                                               \n",
       "Acer_Capillipes               ...         10         10         10         10   \n",
       "Acer_Circinatum               ...         10         10         10         10   \n",
       "Acer_Mono                     ...         10         10         10         10   \n",
       "Acer_Opalus                   ...         10         10         10         10   \n",
       "Acer_Palmatum                 ...         10         10         10         10   \n",
       "...                           ...        ...        ...        ...        ...   \n",
       "Tilia_Tomentosa               ...         10         10         10         10   \n",
       "Ulmus_Bergmanniana            ...         10         10         10         10   \n",
       "Viburnum_Tinus                ...         10         10         10         10   \n",
       "Viburnum_x_Rhytidophylloides  ...         10         10         10         10   \n",
       "Zelkova_Serrata               ...         10         10         10         10   \n",
       "\n",
       "                              texture59  texture60  texture61  texture62  \\\n",
       "species                                                                    \n",
       "Acer_Capillipes                      10         10         10         10   \n",
       "Acer_Circinatum                      10         10         10         10   \n",
       "Acer_Mono                            10         10         10         10   \n",
       "Acer_Opalus                          10         10         10         10   \n",
       "Acer_Palmatum                        10         10         10         10   \n",
       "...                                 ...        ...        ...        ...   \n",
       "Tilia_Tomentosa                      10         10         10         10   \n",
       "Ulmus_Bergmanniana                   10         10         10         10   \n",
       "Viburnum_Tinus                       10         10         10         10   \n",
       "Viburnum_x_Rhytidophylloides         10         10         10         10   \n",
       "Zelkova_Serrata                      10         10         10         10   \n",
       "\n",
       "                              texture63  texture64  \n",
       "species                                             \n",
       "Acer_Capillipes                      10         10  \n",
       "Acer_Circinatum                      10         10  \n",
       "Acer_Mono                            10         10  \n",
       "Acer_Opalus                          10         10  \n",
       "Acer_Palmatum                        10         10  \n",
       "...                                 ...        ...  \n",
       "Tilia_Tomentosa                      10         10  \n",
       "Ulmus_Bergmanniana                   10         10  \n",
       "Viburnum_Tinus                       10         10  \n",
       "Viburnum_x_Rhytidophylloides         10         10  \n",
       "Zelkova_Serrata                      10         10  \n",
       "\n",
       "[99 rows x 192 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(\"species\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870346a6",
   "metadata": {},
   "source": [
    "On voit que pour chaque espèce de plantes, on a exactement 10 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddfc9218",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>margin1</th>\n",
       "      <th>margin2</th>\n",
       "      <th>margin3</th>\n",
       "      <th>margin4</th>\n",
       "      <th>margin5</th>\n",
       "      <th>margin6</th>\n",
       "      <th>margin7</th>\n",
       "      <th>margin8</th>\n",
       "      <th>margin9</th>\n",
       "      <th>margin10</th>\n",
       "      <th>...</th>\n",
       "      <th>texture55</th>\n",
       "      <th>texture56</th>\n",
       "      <th>texture57</th>\n",
       "      <th>texture58</th>\n",
       "      <th>texture59</th>\n",
       "      <th>texture60</th>\n",
       "      <th>texture61</th>\n",
       "      <th>texture62</th>\n",
       "      <th>texture63</th>\n",
       "      <th>texture64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "      <td>990.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.017412</td>\n",
       "      <td>0.028539</td>\n",
       "      <td>0.031988</td>\n",
       "      <td>0.023280</td>\n",
       "      <td>0.014264</td>\n",
       "      <td>0.038579</td>\n",
       "      <td>0.019202</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.007167</td>\n",
       "      <td>0.018639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036501</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>0.015944</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>0.016108</td>\n",
       "      <td>0.014017</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.020291</td>\n",
       "      <td>0.008989</td>\n",
       "      <td>0.019420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.019739</td>\n",
       "      <td>0.038855</td>\n",
       "      <td>0.025847</td>\n",
       "      <td>0.028411</td>\n",
       "      <td>0.018390</td>\n",
       "      <td>0.052030</td>\n",
       "      <td>0.017511</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>0.016071</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063403</td>\n",
       "      <td>0.019321</td>\n",
       "      <td>0.023214</td>\n",
       "      <td>0.025040</td>\n",
       "      <td>0.015335</td>\n",
       "      <td>0.060151</td>\n",
       "      <td>0.011415</td>\n",
       "      <td>0.039040</td>\n",
       "      <td>0.013791</td>\n",
       "      <td>0.022768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.011719</td>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004883</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.011719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.025391</td>\n",
       "      <td>0.041016</td>\n",
       "      <td>0.044922</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.017578</td>\n",
       "      <td>0.056153</td>\n",
       "      <td>0.029297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.027344</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022217</td>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.021484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.012695</td>\n",
       "      <td>0.029297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.087891</td>\n",
       "      <td>0.205080</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.169920</td>\n",
       "      <td>0.111330</td>\n",
       "      <td>0.310550</td>\n",
       "      <td>0.091797</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.076172</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429690</td>\n",
       "      <td>0.202150</td>\n",
       "      <td>0.172850</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.106450</td>\n",
       "      <td>0.578130</td>\n",
       "      <td>0.151370</td>\n",
       "      <td>0.375980</td>\n",
       "      <td>0.086914</td>\n",
       "      <td>0.141600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 192 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          margin1     margin2     margin3     margin4     margin5     margin6  \\\n",
       "count  990.000000  990.000000  990.000000  990.000000  990.000000  990.000000   \n",
       "mean     0.017412    0.028539    0.031988    0.023280    0.014264    0.038579   \n",
       "std      0.019739    0.038855    0.025847    0.028411    0.018390    0.052030   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.001953    0.001953    0.013672    0.005859    0.001953    0.000000   \n",
       "50%      0.009766    0.011719    0.025391    0.013672    0.007812    0.015625   \n",
       "75%      0.025391    0.041016    0.044922    0.029297    0.017578    0.056153   \n",
       "max      0.087891    0.205080    0.156250    0.169920    0.111330    0.310550   \n",
       "\n",
       "          margin7     margin8     margin9    margin10  ...   texture55  \\\n",
       "count  990.000000  990.000000  990.000000  990.000000  ...  990.000000   \n",
       "mean     0.019202    0.001083    0.007167    0.018639  ...    0.036501   \n",
       "std      0.017511    0.002743    0.008933    0.016071  ...    0.063403   \n",
       "min      0.000000    0.000000    0.000000    0.000000  ...    0.000000   \n",
       "25%      0.005859    0.000000    0.001953    0.005859  ...    0.000000   \n",
       "50%      0.015625    0.000000    0.005859    0.015625  ...    0.004883   \n",
       "75%      0.029297    0.000000    0.007812    0.027344  ...    0.043701   \n",
       "max      0.091797    0.031250    0.076172    0.097656  ...    0.429690   \n",
       "\n",
       "        texture56   texture57   texture58   texture59   texture60   texture61  \\\n",
       "count  990.000000  990.000000  990.000000  990.000000  990.000000  990.000000   \n",
       "mean     0.005024    0.015944    0.011586    0.016108    0.014017    0.002688   \n",
       "std      0.019321    0.023214    0.025040    0.015335    0.060151    0.011415   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000977    0.000000    0.004883    0.000000    0.000000   \n",
       "50%      0.000000    0.005859    0.000977    0.012695    0.000000    0.000000   \n",
       "75%      0.000000    0.022217    0.009766    0.021484    0.000000    0.000000   \n",
       "max      0.202150    0.172850    0.200200    0.106450    0.578130    0.151370   \n",
       "\n",
       "        texture62   texture63   texture64  \n",
       "count  990.000000  990.000000  990.000000  \n",
       "mean     0.020291    0.008989    0.019420  \n",
       "std      0.039040    0.013791    0.022768  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000977  \n",
       "50%      0.003906    0.002930    0.011719  \n",
       "75%      0.023438    0.012695    0.029297  \n",
       "max      0.375980    0.086914    0.141600  \n",
       "\n",
       "[8 rows x 192 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1b8909b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9f09b",
   "metadata": {},
   "source": [
    "## Preprocessing des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1730f",
   "metadata": {},
   "source": [
    "On centre-réduit les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bcc009dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X = data.drop(columns=[\"species\"]).values\n",
    "y = data[\"species\"].values\n",
    "X_validation = data_test.copy().values\n",
    "\n",
    "X = scaler.transform(X)\n",
    "X_validation = scaler.transform(X_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45de6a",
   "metadata": {},
   "source": [
    "On sépare nos données en un jeu d'entraînement et un jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8b6293b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7df4df9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(742, 192) (248, 192) (742,) (248,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579a67f3",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cda93a",
   "metadata": {},
   "source": [
    "### Classificateur naïf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704d72cb",
   "metadata": {},
   "source": [
    "Pour avoir une baseline, on va entraîner un classificateur des k plus proches voisins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dcf98e8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=11)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kNN = KNeighborsClassifier(n_neighbors = 11)\n",
    "kNN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a163d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "y_kNN = kNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2c75d718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(248,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_kNN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bcf017ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du classificateur 11-NN : 69.35%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "confusion_mat = pd.DataFrame(confusion_matrix(y_test, y_kNN))\n",
    "accuracy_kNN = accuracy_score(y_test, y_kNN)\n",
    "print(f\"Score du classificateur 11-NN : {accuracy_kNN.round(4)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231c526",
   "metadata": {},
   "source": [
    "En conclusion, notre baseline est de 69.35% d'accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d077ed",
   "metadata": {},
   "source": [
    "### Classificateur SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ac02a",
   "metadata": {},
   "source": [
    "Commençons par tester plusieurs méthodes de Support Vector Machine de base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e675593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "def svc(multiclass):\n",
    "    svc_ = LinearSVC(verbose=0, multi_class=multiclass)\n",
    "    svc_.fit(X_train, y_train)\n",
    "    y_svc_ = svc_.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_svc_)\n",
    "    print(f\"Score du SVC multiclasse {multiclass} : {accuracy.round(4)*100}%\")\n",
    "    \n",
    "    return y_svc_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14acc075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du SVC multiclasse ovr : 74.6%\n",
      "Score du SVC multiclasse crammer_singer : 83.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_svc_ovr = svc(\"ovr\")\n",
    "y_svc_ovo = svc(\"crammer_singer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68710ff9",
   "metadata": {},
   "source": [
    "On voit que les SVC ovr et crammer_singer sont meilleures que le kNN. Voyons si on peut encore faire mieux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "64c041b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LinearSVC in module sklearn.svm._classes:\n",
      "\n",
      "class LinearSVC(sklearn.linear_model._base.LinearClassifierMixin, sklearn.linear_model._base.SparseCoefMixin, sklearn.base.BaseEstimator)\n",
      " |  LinearSVC(penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |  \n",
      " |  Linear Support Vector Classification.\n",
      " |  \n",
      " |  Similar to SVC with parameter kernel='linear', but implemented in terms of\n",
      " |  liblinear rather than libsvm, so it has more flexibility in the choice of\n",
      " |  penalties and loss functions and should scale better to large numbers of\n",
      " |  samples.\n",
      " |  \n",
      " |  This class supports both dense and sparse input and the multiclass support\n",
      " |  is handled according to a one-vs-the-rest scheme.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  penalty : {'l1', 'l2'}, default='l2'\n",
      " |      Specifies the norm used in the penalization. The 'l2'\n",
      " |      penalty is the standard used in SVC. The 'l1' leads to ``coef_``\n",
      " |      vectors that are sparse.\n",
      " |  \n",
      " |  loss : {'hinge', 'squared_hinge'}, default='squared_hinge'\n",
      " |      Specifies the loss function. 'hinge' is the standard SVM loss\n",
      " |      (used e.g. by the SVC class) while 'squared_hinge' is the\n",
      " |      square of the hinge loss. The combination of ``penalty='l1'``\n",
      " |      and ``loss='hinge'`` is not supported.\n",
      " |  \n",
      " |  dual : bool, default=True\n",
      " |      Select the algorithm to either solve the dual or primal\n",
      " |      optimization problem. Prefer dual=False when n_samples > n_features.\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for stopping criteria.\n",
      " |  \n",
      " |  C : float, default=1.0\n",
      " |      Regularization parameter. The strength of the regularization is\n",
      " |      inversely proportional to C. Must be strictly positive.\n",
      " |  \n",
      " |  multi_class : {'ovr', 'crammer_singer'}, default='ovr'\n",
      " |      Determines the multi-class strategy if `y` contains more than\n",
      " |      two classes.\n",
      " |      ``\"ovr\"`` trains n_classes one-vs-rest classifiers, while\n",
      " |      ``\"crammer_singer\"`` optimizes a joint objective over all classes.\n",
      " |      While `crammer_singer` is interesting from a theoretical perspective\n",
      " |      as it is consistent, it is seldom used in practice as it rarely leads\n",
      " |      to better accuracy and is more expensive to compute.\n",
      " |      If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dual\n",
      " |      will be ignored.\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to false, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be already centered).\n",
      " |  \n",
      " |  intercept_scaling : float, default=1\n",
      " |      When self.fit_intercept is True, instance vector x becomes\n",
      " |      ``[x, self.intercept_scaling]``,\n",
      " |      i.e. a \"synthetic\" feature with constant value equals to\n",
      " |      intercept_scaling is appended to the instance vector.\n",
      " |      The intercept becomes intercept_scaling * synthetic feature weight\n",
      " |      Note! the synthetic feature weight is subject to l1/l2 regularization\n",
      " |      as all other features.\n",
      " |      To lessen the effect of regularization on synthetic feature weight\n",
      " |      (and therefore on the intercept) intercept_scaling has to be increased.\n",
      " |  \n",
      " |  class_weight : dict or 'balanced', default=None\n",
      " |      Set the parameter C of class i to ``class_weight[i]*C`` for\n",
      " |      SVC. If not given, all classes are supposed to have\n",
      " |      weight one.\n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. Note that this setting takes advantage of a\n",
      " |      per-process runtime setting in liblinear that, if enabled, may not work\n",
      " |      properly in a multithreaded context.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the pseudo random number generation for shuffling the data for\n",
      " |      the dual coordinate descent (if ``dual=True``). When ``dual=False`` the\n",
      " |      underlying implementation of :class:`LinearSVC` is not random and\n",
      " |      ``random_state`` has no effect on the results.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of iterations to be run.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : ndarray of shape (1, n_features) if n_classes == 2             else (n_classes, n_features)\n",
      " |      Weights assigned to the features (coefficients in the primal\n",
      " |      problem).\n",
      " |  \n",
      " |      ``coef_`` is a readonly property derived from ``raw_coef_`` that\n",
      " |      follows the internal memory layout of liblinear.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The unique classes labels.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      Maximum number of iterations run across all classes.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SVC : Implementation of Support Vector Machine classifier using libsvm:\n",
      " |      the kernel can be non-linear but its SMO algorithm does not\n",
      " |      scale to large number of samples as LinearSVC does.\n",
      " |  \n",
      " |      Furthermore SVC multi-class mode is implemented using one\n",
      " |      vs one scheme while LinearSVC uses one vs the rest. It is\n",
      " |      possible to implement one vs the rest with SVC by using the\n",
      " |      :class:`~sklearn.multiclass.OneVsRestClassifier` wrapper.\n",
      " |  \n",
      " |      Finally SVC can fit dense data without memory copy if the input\n",
      " |      is C-contiguous. Sparse data will still incur memory copy though.\n",
      " |  \n",
      " |  sklearn.linear_model.SGDClassifier : SGDClassifier can optimize the same\n",
      " |      cost function as LinearSVC\n",
      " |      by adjusting the penalty and loss parameters. In addition it requires\n",
      " |      less memory, allows incremental (online) learning, and implements\n",
      " |      various loss functions and regularization regimes.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The underlying C implementation uses a random number generator to\n",
      " |  select features when fitting the model. It is thus not uncommon\n",
      " |  to have slightly different results for the same input data. If\n",
      " |  that happens, try with a smaller ``tol`` parameter.\n",
      " |  \n",
      " |  The underlying implementation, liblinear, uses a sparse internal\n",
      " |  representation for the data that will incur a memory copy.\n",
      " |  \n",
      " |  Predict output may not match that of standalone liblinear in certain\n",
      " |  cases. See :ref:`differences from liblinear <liblinear_differences>`\n",
      " |  in the narrative documentation.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  `LIBLINEAR: A Library for Large Linear Classification\n",
      " |  <https://www.csie.ntu.edu.tw/~cjlin/liblinear/>`__\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.svm import LinearSVC\n",
      " |  >>> from sklearn.pipeline import make_pipeline\n",
      " |  >>> from sklearn.preprocessing import StandardScaler\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> X, y = make_classification(n_features=4, random_state=0)\n",
      " |  >>> clf = make_pipeline(StandardScaler(),\n",
      " |  ...                     LinearSVC(random_state=0, tol=1e-5))\n",
      " |  >>> clf.fit(X, y)\n",
      " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      " |                  ('linearsvc', LinearSVC(random_state=0, tol=1e-05))])\n",
      " |  \n",
      " |  >>> print(clf.named_steps['linearsvc'].coef_)\n",
      " |  [[0.141...   0.526... 0.679... 0.493...]]\n",
      " |  \n",
      " |  >>> print(clf.named_steps['linearsvc'].intercept_)\n",
      " |  [0.1693...]\n",
      " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
      " |  [1]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearSVC\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit the model according to the given training data.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training vector, where `n_samples` is the number of samples and\n",
      " |          `n_features` is the number of features.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target vector relative to X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Array of weights that are assigned to individual\n",
      " |          samples. If not provided,\n",
      " |          then each sample is given unit weight.\n",
      " |      \n",
      " |          .. versionadded:: 0.18\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          An instance of the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the confidence scores.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : ndarray of shape (n_samples,) or (n_samples, n_classes)\n",
      " |          Confidence scores per `(n_samples, n_classes)` combination. In the\n",
      " |          binary case, confidence score for `self.classes_[1]` where >0 means\n",
      " |          this class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The data matrix for which we want to get the predictions.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_pred : ndarray of shape (n_samples,)\n",
      " |          Vector containing the class labels for each sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(LinearSVC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3dabe",
   "metadata": {},
   "source": [
    "### Optimisation des classificateurs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f953c0",
   "metadata": {},
   "source": [
    "Pour optimiser notre SVC, on va procéder par validations croisées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "13eb78af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 56 candidates, totalling 448 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=8.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "112 fits failed out of a total of 448.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "56 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "56 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.82339878 0.82073983 0.01078191 0.11322464\n",
      " 0.84358929 0.83152174        nan        nan 0.88534362 0.88265545\n",
      " 0.01078191 0.35305633 0.88399953 0.88265545        nan        nan\n",
      " 0.88534362 0.88534362 0.01348469 0.57668595 0.88534362 0.88534362\n",
      "        nan        nan 0.88534362 0.88534362 0.37451788 0.83824217\n",
      " 0.88534362 0.88534362        nan        nan 0.90693665 0.90693665\n",
      " 0.90294822 0.92851508 0.90693665 0.90693665        nan        nan\n",
      " 0.94879324 0.94879324 0.91101274 0.94474638 0.94879324 0.94879324\n",
      "        nan        nan 0.94879324 0.94879324 0.94198516 0.94740533\n",
      " 0.94879324 0.94879324]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=8, estimator=LinearSVC(), n_jobs=-1,\n",
       "             param_grid={'C': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                         'dual': [False], 'loss': ['hinge', 'squared_hinge'],\n",
       "                         'multi_class': ['ovr', 'crammer_singer'],\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# On instancie l'estimateur\n",
    "estimator = LinearSVC()\n",
    "\n",
    "# On collecte toutes les valeurs possibles des paramètres\n",
    "params = {\"C\" : np.logspace(-3,3,7),\n",
    "          \"penalty\" : [\"l1\", \"l2\"],\n",
    "          \"loss\" : [\"hinge\", \"squared_hinge\"],\n",
    "          \"dual\" : [False],\n",
    "          \"multi_class\" : [\"ovr\", \"crammer_singer\"]}\n",
    "\n",
    "# On crée la grille de validation croisée\n",
    "grid = GridSearchCV(estimator,\n",
    "                   param_grid = params,\n",
    "                   scoring = \"accuracy\",\n",
    "                   n_jobs = -1,\n",
    "                   cv = 8,\n",
    "                   verbose = 1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "ce015efe",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100.0,\n",
       " 'dual': False,\n",
       " 'loss': 'hinge',\n",
       " 'multi_class': 'crammer_singer',\n",
       " 'penalty': 'l1'}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "9c9cab7a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_dual</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_multi_class</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>13.144056</td>\n",
       "      <td>1.856353</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.006767</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>14.403905</td>\n",
       "      <td>1.873809</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.001653</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>12.482315</td>\n",
       "      <td>1.432313</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.000432</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>13.043231</td>\n",
       "      <td>1.338060</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>12.704581</td>\n",
       "      <td>1.242929</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>12.660453</td>\n",
       "      <td>1.638149</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>12.962851</td>\n",
       "      <td>1.495682</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>13.241497</td>\n",
       "      <td>1.394284</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.948793</td>\n",
       "      <td>0.014993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>7.493438</td>\n",
       "      <td>0.435399</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.016666</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1.798759</td>\n",
       "      <td>0.462088</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>0.005348</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.944746</td>\n",
       "      <td>0.015627</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>15.061094</td>\n",
       "      <td>0.419762</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.941985</td>\n",
       "      <td>0.021702</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.589495</td>\n",
       "      <td>0.319112</td>\n",
       "      <td>0.003501</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.928515</td>\n",
       "      <td>0.032891</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>8.827525</td>\n",
       "      <td>0.194075</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>100.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.911013</td>\n",
       "      <td>0.029479</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6.416982</td>\n",
       "      <td>0.611579</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.000484</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.906937</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6.506489</td>\n",
       "      <td>0.633043</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.906937</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>6.549117</td>\n",
       "      <td>0.469728</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.000501</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.906937</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>7.520598</td>\n",
       "      <td>0.521979</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.906937</td>\n",
       "      <td>0.027204</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>5.545416</td>\n",
       "      <td>0.246820</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>10.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>ovr</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.902948</td>\n",
       "      <td>0.025874</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3.705731</td>\n",
       "      <td>0.315950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.885344</td>\n",
       "      <td>0.029449</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.708272</td>\n",
       "      <td>0.332227</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>crammer_singer</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.885344</td>\n",
       "      <td>0.029449</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "55      13.144056      1.856353         0.003907        0.006767  1000.0   \n",
       "54      14.403905      1.873809         0.002376        0.001653  1000.0   \n",
       "43      12.482315      1.432313         0.001251        0.000432   100.0   \n",
       "46      13.043231      1.338060         0.001126        0.000332   100.0   \n",
       "47      12.704581      1.242929         0.001876        0.000332   100.0   \n",
       "42      12.660453      1.638149         0.001626        0.000484   100.0   \n",
       "51      12.962851      1.495682         0.001501        0.000501  1000.0   \n",
       "50      13.241497      1.394284         0.001501        0.000500  1000.0   \n",
       "53       7.493438      0.435399         0.002126        0.000600  1000.0   \n",
       "45       1.798759      0.462088         0.003876        0.005348   100.0   \n",
       "52      15.061094      0.419762         0.002376        0.000697  1000.0   \n",
       "37       1.589495      0.319112         0.003501        0.004823    10.0   \n",
       "44       8.827525      0.194075         0.001876        0.000600   100.0   \n",
       "39       6.416982      0.611579         0.001625        0.000484    10.0   \n",
       "38       6.506489      0.633043         0.001503        0.000502    10.0   \n",
       "35       6.549117      0.469728         0.001501        0.000501    10.0   \n",
       "34       7.520598      0.521979         0.001126        0.000330    10.0   \n",
       "36       5.545416      0.246820         0.001751        0.000662    10.0   \n",
       "26       3.705731      0.315950         0.000000        0.000000     1.0   \n",
       "31       4.708272      0.332227         0.001953        0.005167     1.0   \n",
       "\n",
       "   param_dual     param_loss param_multi_class param_penalty  mean_test_score  \\\n",
       "55      False  squared_hinge    crammer_singer            l2         0.948793   \n",
       "54      False  squared_hinge    crammer_singer            l1         0.948793   \n",
       "43      False          hinge    crammer_singer            l2         0.948793   \n",
       "46      False  squared_hinge    crammer_singer            l1         0.948793   \n",
       "47      False  squared_hinge    crammer_singer            l2         0.948793   \n",
       "42      False          hinge    crammer_singer            l1         0.948793   \n",
       "51      False          hinge    crammer_singer            l2         0.948793   \n",
       "50      False          hinge    crammer_singer            l1         0.948793   \n",
       "53      False  squared_hinge               ovr            l2         0.947405   \n",
       "45      False  squared_hinge               ovr            l2         0.944746   \n",
       "52      False  squared_hinge               ovr            l1         0.941985   \n",
       "37      False  squared_hinge               ovr            l2         0.928515   \n",
       "44      False  squared_hinge               ovr            l1         0.911013   \n",
       "39      False  squared_hinge    crammer_singer            l2         0.906937   \n",
       "38      False  squared_hinge    crammer_singer            l1         0.906937   \n",
       "35      False          hinge    crammer_singer            l2         0.906937   \n",
       "34      False          hinge    crammer_singer            l1         0.906937   \n",
       "36      False  squared_hinge               ovr            l1         0.902948   \n",
       "26      False          hinge    crammer_singer            l1         0.885344   \n",
       "31      False  squared_hinge    crammer_singer            l2         0.885344   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "55        0.014993                1  \n",
       "54        0.014993                1  \n",
       "43        0.014993                1  \n",
       "46        0.014993                1  \n",
       "47        0.014993                1  \n",
       "42        0.014993                1  \n",
       "51        0.014993                1  \n",
       "50        0.014993                1  \n",
       "53        0.016666                9  \n",
       "45        0.015627               10  \n",
       "52        0.021702               11  \n",
       "37        0.032891               12  \n",
       "44        0.029479               13  \n",
       "39        0.027204               14  \n",
       "38        0.027204               14  \n",
       "35        0.027204               14  \n",
       "34        0.027204               14  \n",
       "36        0.025874               18  \n",
       "26        0.029449               19  \n",
       "31        0.029449               19  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results = results.drop(columns = [i for i in results.columns if \"split\" in i]).drop(columns = [\"params\"]).sort_values(\"rank_test_score\")\n",
    "results.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b890e68",
   "metadata": {},
   "source": [
    "Fort de ces résultats, on va entraîner deux nouveaux classificateurs :\n",
    "- Un qui suit la méthode One-vs-Rest\n",
    "- Un qui suit la méthode Crammer & Singer\n",
    "\n",
    "et on va les tester sur le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f482852f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du SVC ovr : 92.34%\n"
     ]
    }
   ],
   "source": [
    "svc_ovr = LinearSVC(dual=False, penalty=\"l2\", loss=\"squared_hinge\", C = 5000, multi_class=\"ovr\")\n",
    "svc_ovr.fit(X_train, y_train)\n",
    "y_pred_ovr = svc_ovr.predict(X_test)\n",
    "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
    "print(f\"Score du SVC ovr : {accuracy_ovr.round(4)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dd2c8fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du SVC crammer singer : 90.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "svc_crammer_singer = LinearSVC(C = 10000, multi_class=\"crammer_singer\")\n",
    "svc_crammer_singer.fit(X_train, y_train)\n",
    "y_pred_crammer_singer = svc_crammer_singer.predict(X_test)\n",
    "accuracy_crammer_singer = accuracy_score(y_test, y_pred_crammer_singer)\n",
    "print(f\"Score du SVC crammer singer : {accuracy_crammer_singer.round(4)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15a8061",
   "metadata": {},
   "source": [
    "C'est sans appel, le SVC ovr est meilleur que le svc crammer singer. Cependant, en tâtonnant un peu, on se rend compte que l'on avait mal géré les valeurs de C. Refaisons une validation croisée plus précise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "42283fcb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 8 folds for each of 40 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:676: UserWarning: The least populated class in y has only 4 members, which is less than n_splits=8.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "160 fits failed out of a total of 320.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "80 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_classes.py\", line 257, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1185, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1024, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [       nan        nan 0.90832457 0.94474638        nan        nan\n",
      " 0.92178004 0.94877863        nan        nan 0.92045056 0.94607585\n",
      "        nan        nan 0.92714177 0.94740533        nan        nan\n",
      " 0.93117403 0.94740533        nan        nan 0.94195594 0.94740533\n",
      "        nan        nan 0.95817263 0.94470255        nan        nan\n",
      " 0.9514522  0.94470255        nan        nan 0.95682854 0.94470255\n",
      "        nan        nan 0.95548446 0.94470255]\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=8, estimator=LinearSVC(dual=False), n_jobs=-1,\n",
       "             param_grid={'C': array([  100.        ,   166.81005372,   278.25594022,   464.15888336,\n",
       "         774.26368268,  1291.54966501,  2154.43469003,  3593.8136638 ,\n",
       "        5994.84250319, 10000.        ]),\n",
       "                         'loss': ['hinge', 'squared_hinge'],\n",
       "                         'penalty': ['l1', 'l2']},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On instancie l'estimateur\n",
    "estimator = LinearSVC(multi_class=\"ovr\", dual=False)\n",
    "\n",
    "# On collecte toutes les valeurs possibles des paramètres\n",
    "params = {\"C\" : np.logspace(2, 4, 10),\n",
    "          \"penalty\" : [\"l1\", \"l2\"],\n",
    "          \"loss\" : [\"hinge\", \"squared_hinge\"]}\n",
    "\n",
    "# On crée la grille de validation croisée\n",
    "grid = GridSearchCV(estimator,\n",
    "                   param_grid = params,\n",
    "                   scoring = \"accuracy\",\n",
    "                   n_jobs = -1,\n",
    "                   cv = 8,\n",
    "                   verbose = 1)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2273d179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 2154.4346900318824, 'loss': 'squared_hinge', 'penalty': 'l1'}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b2a1193c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_loss</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>20.775519</td>\n",
       "      <td>0.680829</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>0.009904</td>\n",
       "      <td>2154.43469</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.958173</td>\n",
       "      <td>0.017573</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>24.576707</td>\n",
       "      <td>1.297229</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>5994.842503</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.956829</td>\n",
       "      <td>0.018769</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>28.340716</td>\n",
       "      <td>1.044101</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.006764</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.955484</td>\n",
       "      <td>0.012784</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>26.102233</td>\n",
       "      <td>1.134269</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>3593.813664</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.951452</td>\n",
       "      <td>0.013356</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.960277</td>\n",
       "      <td>0.276260</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>166.810054</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.948779</td>\n",
       "      <td>0.015017</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.367803</td>\n",
       "      <td>0.614506</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>774.263683</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.016666</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8.781134</td>\n",
       "      <td>1.174209</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.007564</td>\n",
       "      <td>1291.549665</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.016666</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.922822</td>\n",
       "      <td>1.178416</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>464.158883</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.947405</td>\n",
       "      <td>0.016666</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6.812674</td>\n",
       "      <td>0.940984</td>\n",
       "      <td>0.003981</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>278.25594</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.946076</td>\n",
       "      <td>0.016229</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.466245</td>\n",
       "      <td>0.132033</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.005829</td>\n",
       "      <td>100.0</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.944746</td>\n",
       "      <td>0.015627</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9.413611</td>\n",
       "      <td>1.670670</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>5994.842503</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.944703</td>\n",
       "      <td>0.019115</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>11.609897</td>\n",
       "      <td>1.311095</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>3593.813664</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.944703</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>13.482903</td>\n",
       "      <td>1.133769</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>2154.43469</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.944703</td>\n",
       "      <td>0.016694</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>6.834863</td>\n",
       "      <td>1.217823</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.944703</td>\n",
       "      <td>0.018344</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>15.818064</td>\n",
       "      <td>0.284087</td>\n",
       "      <td>0.002253</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>1291.549665</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.941956</td>\n",
       "      <td>0.025542</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14.525300</td>\n",
       "      <td>0.607030</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>774.263683</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.931174</td>\n",
       "      <td>0.026116</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16.543982</td>\n",
       "      <td>2.290588</td>\n",
       "      <td>0.002877</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>464.158883</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.927142</td>\n",
       "      <td>0.029674</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11.696297</td>\n",
       "      <td>0.332313</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>166.810054</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.921780</td>\n",
       "      <td>0.022928</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>14.624554</td>\n",
       "      <td>1.331485</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>278.25594</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.920451</td>\n",
       "      <td>0.020962</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.814379</td>\n",
       "      <td>0.128738</td>\n",
       "      <td>0.019601</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>100.0</td>\n",
       "      <td>squared_hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>0.908325</td>\n",
       "      <td>0.027667</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.006678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.006766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5994.842503</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.009766</td>\n",
       "      <td>0.007565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5994.842503</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014374</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.810054</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.006627</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>166.810054</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.007450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3593.813664</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.016873</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.25594</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.013752</td>\n",
       "      <td>0.012205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>464.158883</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.013672</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2154.43469</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.007851</td>\n",
       "      <td>0.007851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2154.43469</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.007751</td>\n",
       "      <td>0.003229</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>278.25594</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.001867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1291.549665</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.008876</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1291.549665</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>774.263683</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.009499</td>\n",
       "      <td>0.005027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>774.263683</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.007377</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>464.158883</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.010543</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3593.813664</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005273</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>hinge</td>\n",
       "      <td>l1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time      param_C  \\\n",
       "26      20.775519      0.680829         0.005283        0.009904   2154.43469   \n",
       "34      24.576707      1.297229         0.003907        0.006768  5994.842503   \n",
       "38      28.340716      1.044101         0.003905        0.006764      10000.0   \n",
       "30      26.102233      1.134269         0.001953        0.005168  3593.813664   \n",
       "7        5.960277      0.276260         0.002501        0.001119   166.810054   \n",
       "19       7.367803      0.614506         0.002000        0.000708   774.263683   \n",
       "23       8.781134      1.174209         0.005859        0.007564  1291.549665   \n",
       "15       5.922822      1.178416         0.002251        0.000829   464.158883   \n",
       "11       6.812674      0.940984         0.003981        0.003227    278.25594   \n",
       "3        5.466245      0.132033         0.004629        0.005829        100.0   \n",
       "35       9.413611      1.670670         0.000500        0.000708  5994.842503   \n",
       "31      11.609897      1.311095         0.001953        0.005168  3593.813664   \n",
       "27      13.482903      1.133769         0.001502        0.001002   2154.43469   \n",
       "39       6.834863      1.217823         0.000000        0.000000      10000.0   \n",
       "22      15.818064      0.284087         0.002253        0.000663  1291.549665   \n",
       "18      14.525300      0.607030         0.002252        0.000661   774.263683   \n",
       "14      16.543982      2.290588         0.002877        0.001166   464.158883   \n",
       "6       11.696297      0.332313         0.003106        0.002335   166.810054   \n",
       "10      14.624554      1.331485         0.004248        0.001559    278.25594   \n",
       "2        8.814379      0.128738         0.019601        0.004530        100.0   \n",
       "37       0.004085      0.006678         0.000000        0.000000      10000.0   \n",
       "36       0.009766      0.007565         0.000000        0.000000      10000.0   \n",
       "1        0.003906      0.006766         0.000000        0.000000        100.0   \n",
       "33       0.000000      0.000000         0.000000        0.000000  5994.842503   \n",
       "32       0.009766      0.007565         0.000000        0.000000  5994.842503   \n",
       "4        0.014374      0.016695         0.000000        0.000000   166.810054   \n",
       "5        0.006627      0.001218         0.000000        0.000000   166.810054   \n",
       "29       0.013002      0.007450         0.000000        0.000000  3593.813664   \n",
       "8        0.016873      0.007994         0.000000        0.000000    278.25594   \n",
       "13       0.013752      0.012205         0.000000        0.000000   464.158883   \n",
       "25       0.013672      0.005168         0.000000        0.000000   2154.43469   \n",
       "24       0.007851      0.007851         0.000000        0.000000   2154.43469   \n",
       "9        0.007751      0.003229         0.000000        0.000000    278.25594   \n",
       "21       0.008372      0.001867         0.000000        0.000000  1291.549665   \n",
       "20       0.008876      0.004076         0.000000        0.000000  1291.549665   \n",
       "17       0.009625      0.006889         0.000000        0.000000   774.263683   \n",
       "16       0.009499      0.005027         0.000000        0.000000   774.263683   \n",
       "12       0.007377      0.002234         0.000000        0.000000   464.158883   \n",
       "28       0.010543      0.006588         0.000000        0.000000  3593.813664   \n",
       "0        0.005273      0.002781         0.000000        0.000000        100.0   \n",
       "\n",
       "       param_loss param_penalty  mean_test_score  std_test_score  \\\n",
       "26  squared_hinge            l1         0.958173        0.017573   \n",
       "34  squared_hinge            l1         0.956829        0.018769   \n",
       "38  squared_hinge            l1         0.955484        0.012784   \n",
       "30  squared_hinge            l1         0.951452        0.013356   \n",
       "7   squared_hinge            l2         0.948779        0.015017   \n",
       "19  squared_hinge            l2         0.947405        0.016666   \n",
       "23  squared_hinge            l2         0.947405        0.016666   \n",
       "15  squared_hinge            l2         0.947405        0.016666   \n",
       "11  squared_hinge            l2         0.946076        0.016229   \n",
       "3   squared_hinge            l2         0.944746        0.015627   \n",
       "35  squared_hinge            l2         0.944703        0.019115   \n",
       "31  squared_hinge            l2         0.944703        0.016694   \n",
       "27  squared_hinge            l2         0.944703        0.016694   \n",
       "39  squared_hinge            l2         0.944703        0.018344   \n",
       "22  squared_hinge            l1         0.941956        0.025542   \n",
       "18  squared_hinge            l1         0.931174        0.026116   \n",
       "14  squared_hinge            l1         0.927142        0.029674   \n",
       "6   squared_hinge            l1         0.921780        0.022928   \n",
       "10  squared_hinge            l1         0.920451        0.020962   \n",
       "2   squared_hinge            l1         0.908325        0.027667   \n",
       "37          hinge            l2              NaN             NaN   \n",
       "36          hinge            l1              NaN             NaN   \n",
       "1           hinge            l2              NaN             NaN   \n",
       "33          hinge            l2              NaN             NaN   \n",
       "32          hinge            l1              NaN             NaN   \n",
       "4           hinge            l1              NaN             NaN   \n",
       "5           hinge            l2              NaN             NaN   \n",
       "29          hinge            l2              NaN             NaN   \n",
       "8           hinge            l1              NaN             NaN   \n",
       "13          hinge            l2              NaN             NaN   \n",
       "25          hinge            l2              NaN             NaN   \n",
       "24          hinge            l1              NaN             NaN   \n",
       "9           hinge            l2              NaN             NaN   \n",
       "21          hinge            l2              NaN             NaN   \n",
       "20          hinge            l1              NaN             NaN   \n",
       "17          hinge            l2              NaN             NaN   \n",
       "16          hinge            l1              NaN             NaN   \n",
       "12          hinge            l1              NaN             NaN   \n",
       "28          hinge            l1              NaN             NaN   \n",
       "0           hinge            l1              NaN             NaN   \n",
       "\n",
       "    rank_test_score  \n",
       "26                1  \n",
       "34                2  \n",
       "38                3  \n",
       "30                4  \n",
       "7                 5  \n",
       "19                6  \n",
       "23                6  \n",
       "15                6  \n",
       "11                9  \n",
       "3                10  \n",
       "35               11  \n",
       "31               11  \n",
       "27               11  \n",
       "39               11  \n",
       "22               15  \n",
       "18               16  \n",
       "14               17  \n",
       "6                18  \n",
       "10               19  \n",
       "2                20  \n",
       "37               21  \n",
       "36               22  \n",
       "1                23  \n",
       "33               24  \n",
       "32               25  \n",
       "4                26  \n",
       "5                27  \n",
       "29               28  \n",
       "8                29  \n",
       "13               30  \n",
       "25               31  \n",
       "24               32  \n",
       "9                33  \n",
       "21               34  \n",
       "20               35  \n",
       "17               36  \n",
       "16               37  \n",
       "12               38  \n",
       "28               39  \n",
       "0                40  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(grid.cv_results_)\n",
    "results.drop(columns= [i for i in results.columns if \"split\" in i or \"params\" in i]).sort_values(\"rank_test_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6294f2",
   "metadata": {},
   "source": [
    "On voit qu'on a les meilleurs résultats lorsque la loss est la squared_hinge, C est dans [2200, 3600, 6000] et la penalty = l1. On va donc entraîner ces modèles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5b911b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(C_value):\n",
    "    svc_ovr = LinearSVC(dual=False, penalty=\"l2\", loss=\"squared_hinge\", C = C_value, multi_class=\"ovr\")\n",
    "    svc_ovr.fit(X_train, y_train)\n",
    "    y_pred_ovr = svc_ovr.predict(X_test)\n",
    "    accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
    "    print(f\"Score du SVC ovr avec C = {C_value} : {accuracy_ovr.round(6)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a0a2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du SVC ovr avec C = 2000 : 91.9355%\n"
     ]
    }
   ],
   "source": [
    "f(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5d1d9aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du SVC ovr avec C = 3600 : 92.33869999999999%\n"
     ]
    }
   ],
   "source": [
    "f(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "5625e4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score du SVC ovr avec C = 6000 : 92.33869999999999%\n"
     ]
    }
   ],
   "source": [
    "f(6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f25c6",
   "metadata": {},
   "source": [
    "Pour finir, on va donc prendre C = 2000, loss = \"squared_hinge\", penalty=\"l2\", dual=False, multi_class=\"ovr\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd45fc1",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6142e9f2",
   "metadata": {},
   "source": [
    "En guise de conclusion, on va prédire sur le jeu de validation les étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c22b6632",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.95310000e-02,  9.76600000e-03,  7.81250000e-02, ...,\n",
       "         1.18423789e-16,  3.90600000e-03,  5.37110000e-02],\n",
       "       [ 7.81200000e-03,  5.85900000e-03,  6.44530000e-02, ...,\n",
       "         9.77000000e-04,  3.71090000e-02,  4.49220000e-02],\n",
       "       [ 4.12688963e-17, -9.68921912e-17,  1.95300000e-03, ...,\n",
       "         1.56250000e-02, -1.79429984e-17,  4.12688963e-17],\n",
       "       ...,\n",
       "       [ 1.75780000e-02,  2.92970000e-02,  1.56250000e-02, ...,\n",
       "         1.18423789e-16,  4.29690000e-02,  6.83600000e-03],\n",
       "       [ 1.36720000e-02,  9.76600000e-03,  6.05470000e-02, ...,\n",
       "         1.18423789e-16,  1.17190000e-02,  1.85550000e-02],\n",
       "       [ 4.12688963e-17,  1.17190000e-01,  1.47132587e-16, ...,\n",
       "         1.56250000e-02, -1.79429984e-17,  1.75780000e-02]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f15a80b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearSVC(dual=False, penalty=\"l2\", loss=\"squared_hinge\", C = 2000, multi_class=\"ovr\")\n",
    "model.fit(X, y)\n",
    "y_validation_pred = model.predict(X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ded6897a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y, model.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "894bba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Prunus_Avium'] ['Prunus_Avium']\n"
     ]
    }
   ],
   "source": [
    "X_0 = data.loc[data.index==456].drop(columns=[\"species\"]).values\n",
    "y_0 = data.loc[data.index==456].species.values\n",
    "\n",
    "print(y_0, model.predict(X_0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca7270",
   "metadata": {},
   "source": [
    "Bon, ça marche nickel ! Par contre, je me demande dans quelle mesure j'ai pas fait de surapprentrissage ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
